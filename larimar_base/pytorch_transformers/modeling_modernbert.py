#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨
#           This file was automatically generated from src/transformers/models/modernbert/modular_modernbert.py.
#               Do NOT edit this file manually as any edits will be overwritten by the generation of
#             the file from the modular. If any change should be done, please apply the change to the
#                          modular_modernbert.py file directly. One of our CI enforces this.
#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨
# Copyright 2024 Answer.AI, LightOn, and contributors, and the HuggingFace Inc. team. All rights reserved.
#
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from typing import Dict, Optional, Tuple, Union

import torch
from torch import nn
from transformers import ModernBertPreTrainedModel, ModernBertModel

from .utils import (
    logging,
)

logger = logging.get_logger(__name__)

_CHECKPOINT_FOR_DOC = "Alibaba-NLP/gte-modernbert-base"
_CONFIG_FOR_DOC = "ModernBertConfig"


class ModernBertForLatentConnector(ModernBertPreTrainedModel):
    """
    ModernBert Model with latent connector for VAE integration, allowing it to be used as an encoder in the Larimar
    framework.
    """
    base_model_prefix = "bert"
    def __init__(self, config, latent_size=32):
        super().__init__(config)
        self.config = config
        self.config = self._autoset_attn_implementation(config)
        self.bert = ModernBertModel(config)
        self.linear = nn.Linear(config.hidden_size, 2 * latent_size)
        self.latent_size = latent_size
        self.post_init()
    
    def get_input_embeddings(self):
        return self.bert.get_input_embeddings()
    
    def set_input_embeddings(self, value):
        self.bert.set_input_embeddings(value)
    
    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        sliding_window_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        inputs_embeds: Optional[torch.Tensor] = None,
        indices: Optional[torch.Tensor] = None,
        cu_seqlens: Optional[torch.Tensor] = None,
        max_seqlen: Optional[int] = None,
        batch_size: Optional[int] = None,
        seq_len: Optional[int] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
    ) -> Union[Tuple[torch.Tensor, ...], Dict[str, torch.Tensor]]:
        """
        Forward pass for ModernBertForLatentConnector.
        
        Returns:
            tuple or dict: If return_dict is False, returns a tuple containing:
                - mu: Mean vector for the VAE
                - logvar: Log variance vector for the VAE
                - Outputs from the ModernBert model
            If return_dict is True, returns a dictionary with the same elements.
        """
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict
        self._maybe_set_compile()
        
        outputs = self.bert(
            input_ids=input_ids,
            attention_mask=attention_mask,
            sliding_window_mask=sliding_window_mask,
            position_ids=position_ids,
            inputs_embeds=inputs_embeds,
            indices=indices,
            cu_seqlens=cu_seqlens,
            max_seqlen=max_seqlen,
            batch_size=batch_size,
            seq_len=seq_len,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=True,
        )
        
        last_hidden_state = outputs.last_hidden_state
        pooled_output = last_hidden_state[:, 0]
        
        projected = self.linear(pooled_output)
        mu, logvar = projected[:, :self.latent_size], projected[:, self.latent_size:]
        
        if not return_dict:
            return mu, logvar, outputs.last_hidden_state, outputs.hidden_states, outputs.attentions
        
        return {
            "mu": mu,
            "logvar": logvar,
            "last_hidden_state": outputs.last_hidden_state,
            "hidden_states": outputs.hidden_states,
            "attentions": outputs.attentions
        }

